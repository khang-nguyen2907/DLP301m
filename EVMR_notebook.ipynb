{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"EVMR_notebook.ipynb","provenance":[],"machine_shape":"hm","mount_file_id":"1Got2QQYISjCKfV5NiTsCGlJ_7UNiobXR","authorship_tag":"ABX9TyPuwS4Hfg6wFjjiQ02wZKlE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"59a2acc525e848ac870e74f3992805ea":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0220a6168543473995d1aac3a9cffdda","IPY_MODEL_9562084da8eb48a88a5363bc964e4a6b","IPY_MODEL_b907cd9e68a144cf9e31f1274cc506ca"],"layout":"IPY_MODEL_fe2489e977ad40e8be192947492d5e21"}},"0220a6168543473995d1aac3a9cffdda":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b3a5e8e80db6428798289d6ff4df76e8","placeholder":"​","style":"IPY_MODEL_0cea9a36a1fc4769ac29db947ccaf1f3","value":"Skipping the first batches: 100%"}},"9562084da8eb48a88a5363bc964e4a6b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7d76e25871944c992949316473f8d6e","max":1120,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2850648452bb4e318985b445d6139f17","value":1120}},"b907cd9e68a144cf9e31f1274cc506ca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f44a3e33af5c4cbfb954c78048ab63b0","placeholder":"​","style":"IPY_MODEL_0238214572d44ca391dec89ae3f25be1","value":" 1120/1120 [01:44&lt;00:00, 11.22it/s]"}},"fe2489e977ad40e8be192947492d5e21":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3a5e8e80db6428798289d6ff4df76e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0cea9a36a1fc4769ac29db947ccaf1f3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d7d76e25871944c992949316473f8d6e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2850648452bb4e318985b445d6139f17":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f44a3e33af5c4cbfb954c78048ab63b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0238214572d44ca391dec89ae3f25be1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"59a117098be5480fa73fb23f02aace4d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1f9a09aa2ac74be79bf4f949badd9194","IPY_MODEL_caa34401fef949eeb3814d88a600df5f","IPY_MODEL_aaea9a66eeff49c68f004c421ab7fd0b"],"layout":"IPY_MODEL_dda80d4c1ce6433192da71c537bcf909"}},"1f9a09aa2ac74be79bf4f949badd9194":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f86a483306949208d37b157166078e7","placeholder":"​","style":"IPY_MODEL_2eed18aec69e499b854454a75a8051b9","value":"Downloading: 100%"}},"caa34401fef949eeb3814d88a600df5f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a4523a59a9c400c8859037a0d85b0e4","max":1596,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6c0070dd63f14a58ab807318a50c13e1","value":1596}},"aaea9a66eeff49c68f004c421ab7fd0b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_58f97b62f4da4e9a911e2f2513c9af62","placeholder":"​","style":"IPY_MODEL_8bd8a9c577154997aebdf88ae053f431","value":" 1.56k/1.56k [00:00&lt;00:00, 66.6kB/s]"}},"dda80d4c1ce6433192da71c537bcf909":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f86a483306949208d37b157166078e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2eed18aec69e499b854454a75a8051b9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0a4523a59a9c400c8859037a0d85b0e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c0070dd63f14a58ab807318a50c13e1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"58f97b62f4da4e9a911e2f2513c9af62":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8bd8a9c577154997aebdf88ae053f431":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0de2b7bfae93446abec47403a58d25d8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f9ac224b9eab48088f9228152c83933f","IPY_MODEL_838d752b04b148cca7675e0d49524cc5","IPY_MODEL_80b00fd1482d484b9fb5722746144e2d"],"layout":"IPY_MODEL_86a2a26fdfdc4073b3dc26ba63416421"}},"f9ac224b9eab48088f9228152c83933f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a146cfa3953746d398fbb376b80733af","placeholder":"​","style":"IPY_MODEL_33ce54732c1744a882c619e49496c874","value":"Downloading: 100%"}},"838d752b04b148cca7675e0d49524cc5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_041123f383134ef6bc58d12f786dbe5c","max":159,"min":0,"orientation":"horizontal","style":"IPY_MODEL_971d1a4b5496434696bcfdb07ddb48f8","value":159}},"80b00fd1482d484b9fb5722746144e2d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac72822647d5430999b519113c727e77","placeholder":"​","style":"IPY_MODEL_ca79a02a466e47d0b5608c02fc4e452a","value":" 159/159 [00:00&lt;00:00, 5.69kB/s]"}},"86a2a26fdfdc4073b3dc26ba63416421":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a146cfa3953746d398fbb376b80733af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33ce54732c1744a882c619e49496c874":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"041123f383134ef6bc58d12f786dbe5c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"971d1a4b5496434696bcfdb07ddb48f8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ac72822647d5430999b519113c727e77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca79a02a466e47d0b5608c02fc4e452a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"73ef4152219443a7ab96fecf6c3635dc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_248442a2a85b41daaca2722d6d3edf29","IPY_MODEL_2dcd96ef4d284909bdcd63696f0a37d4","IPY_MODEL_23b8f614e3544d6eb2d9d2366bd9621a"],"layout":"IPY_MODEL_5ed60b5370c7446eb99b81eb588086e8"}},"248442a2a85b41daaca2722d6d3edf29":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6aad1cce447495ebc7ed8689126e551","placeholder":"​","style":"IPY_MODEL_0a55efdb77604b67bae59f096792b80f","value":"Downloading: 100%"}},"2dcd96ef4d284909bdcd63696f0a37d4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_aab6a449e38f457c8e80d1d4b34f517e","max":163,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0f8d54b9fcab416ea77bab388c24ef13","value":163}},"23b8f614e3544d6eb2d9d2366bd9621a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_12f2d64730744a7eabd988465dfa3f34","placeholder":"​","style":"IPY_MODEL_a19718dda6974f4ab44a8d69f7c2a9dd","value":" 163/163 [00:00&lt;00:00, 4.22kB/s]"}},"5ed60b5370c7446eb99b81eb588086e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6aad1cce447495ebc7ed8689126e551":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a55efdb77604b67bae59f096792b80f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aab6a449e38f457c8e80d1d4b34f517e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f8d54b9fcab416ea77bab388c24ef13":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"12f2d64730744a7eabd988465dfa3f34":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a19718dda6974f4ab44a8d69f7c2a9dd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6746c51e619548b0b6e0138641ae8410":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_69c3c64135b64ea2bc47d26513421b8d","IPY_MODEL_19c0ceca5e3645029af7d52603b909bd","IPY_MODEL_f6b9932833d24a12ac56ae4435023db0"],"layout":"IPY_MODEL_c45267bf288d4149a2892071292b781a"}},"69c3c64135b64ea2bc47d26513421b8d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_34c2f89df12244fe8bdfeb659d0576b0","placeholder":"​","style":"IPY_MODEL_fa4ddcbe85a849d5a40fc8d32d0a01ab","value":"Downloading: 100%"}},"19c0ceca5e3645029af7d52603b909bd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_249c3f84584840aa9d3904ec2d35ae13","max":291,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d5bdbe7c25e7476f989fb3d47aa2cead","value":291}},"f6b9932833d24a12ac56ae4435023db0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7a6786b71dc40baa65951bff2a89e8f","placeholder":"​","style":"IPY_MODEL_349aeea742aa4436bd9b39fadb7ed070","value":" 291/291 [00:00&lt;00:00, 10.0kB/s]"}},"c45267bf288d4149a2892071292b781a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34c2f89df12244fe8bdfeb659d0576b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa4ddcbe85a849d5a40fc8d32d0a01ab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"249c3f84584840aa9d3904ec2d35ae13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5bdbe7c25e7476f989fb3d47aa2cead":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b7a6786b71dc40baa65951bff2a89e8f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"349aeea742aa4436bd9b39fadb7ed070":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3e0db66f059c4559b5cf9a4f8068dc17":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6b5fccd428484bcbbd1448964fe86b3d","IPY_MODEL_efa6e1aeb4cd42a9a2deb6d74b0ed615","IPY_MODEL_4e1a906c3c384f2e97fcb5ed1290033d"],"layout":"IPY_MODEL_f2b653c16c0c43a1911ce48eca7a3de5"}},"6b5fccd428484bcbbd1448964fe86b3d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5fd8a23cb0324b4ca6e7422f7260bcb8","placeholder":"​","style":"IPY_MODEL_e48144f93af4499294b2c94a852f4076","value":"Downloading: 100%"}},"efa6e1aeb4cd42a9a2deb6d74b0ed615":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0b9fd9e5f3a4c72b12aaea51836b25c","max":85,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b619623d07af4b7795f20c688292b1f8","value":85}},"4e1a906c3c384f2e97fcb5ed1290033d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_13c993b71f794ccc94a6aa911263821c","placeholder":"​","style":"IPY_MODEL_800b35759fbe4ae29508bf9ef0d3f98c","value":" 85.0/85.0 [00:00&lt;00:00, 3.20kB/s]"}},"f2b653c16c0c43a1911ce48eca7a3de5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5fd8a23cb0324b4ca6e7422f7260bcb8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e48144f93af4499294b2c94a852f4076":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d0b9fd9e5f3a4c72b12aaea51836b25c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b619623d07af4b7795f20c688292b1f8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"13c993b71f794ccc94a6aa911263821c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"800b35759fbe4ae29508bf9ef0d3f98c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"915c5c2e46464f5f9b79bfe79b4494c6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_80c4b41c11e64edcb96b4dd7ac112b38","IPY_MODEL_fc377f36a02748eba82df90d8936758b","IPY_MODEL_dfce675144d64ffcab72c615409bdf09"],"layout":"IPY_MODEL_5f9f5c0baad049d68f74970b741b3efd"}},"80c4b41c11e64edcb96b4dd7ac112b38":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0764a5036a8349f19867847237531f06","placeholder":"​","style":"IPY_MODEL_46a73beb156746c48083dac10178221e","value":"Downloading: 100%"}},"fc377f36a02748eba82df90d8936758b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d37417050b24c21a8d0a7f3a8ed1eb2","max":377667514,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8e8236edcefc43489da42dd85aad4031","value":377667514}},"dfce675144d64ffcab72c615409bdf09":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_73e4e2bc253f4599a782bf26b54c2cb4","placeholder":"​","style":"IPY_MODEL_d8623f826ac047da94164888b133e25a","value":" 360M/360M [00:08&lt;00:00, 47.1MB/s]"}},"5f9f5c0baad049d68f74970b741b3efd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0764a5036a8349f19867847237531f06":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46a73beb156746c48083dac10178221e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3d37417050b24c21a8d0a7f3a8ed1eb2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e8236edcefc43489da42dd85aad4031":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"73e4e2bc253f4599a782bf26b54c2cb4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8623f826ac047da94164888b133e25a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c529c75724354860bc456c8a92d36b4e":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_bd4e056a9e9b49d69dcc45191e589d13","IPY_MODEL_740adc4d80ab4b1badfd7177361b513f"],"layout":"IPY_MODEL_7e3abf9cb3e845b8bf38f15bcdfae6ac"}},"bd4e056a9e9b49d69dcc45191e589d13":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_89d6ab92e661451a86f06376efa267d8","placeholder":"​","style":"IPY_MODEL_3be840385ef247a1967a65e3a04718a8","value":"362.877 MB of 362.877 MB uploaded (0.000 MB deduped)\r"}},"740adc4d80ab4b1badfd7177361b513f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_573ec482a42a4ef1b72857bf987a1a2c","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8764da494b8843d49597cdf9cff32aa5","value":1}},"7e3abf9cb3e845b8bf38f15bcdfae6ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89d6ab92e661451a86f06376efa267d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3be840385ef247a1967a65e3a04718a8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"573ec482a42a4ef1b72857bf987a1a2c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8764da494b8843d49597cdf9cff32aa5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# INSTALL "],"metadata":{"id":"bvnAgEYV1F3i"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2rMHaE2ez23U"},"outputs":[],"source":["%%capture\n","\n","!pip install git+https://github.com/huggingface/datasets.git\n","!pip install git+https://github.com/huggingface/transformers.git\n","!pip install jiwer\n","!pip install torchaudio\n","!pip install librosa\n","# Monitor the training process\n","# !pip install wandb"]},{"cell_type":"code","source":["!pip install wandb -qqq\n","import wandb"],"metadata":{"id":"QhG0cxFO1CTe","executionInfo":{"status":"ok","timestamp":1655429690485,"user_tz":-420,"elapsed":10751,"user":{"displayName":"Nguyen Hoang Gia Khang (K15 HCM)","userId":"13453407859919139403"}},"outputId":"792d391a-645d-4152-ffc4-92ebfad16718","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 1.8 MB 4.3 MB/s \n","\u001b[K     |████████████████████████████████| 145 kB 68.0 MB/s \n","\u001b[K     |████████████████████████████████| 181 kB 74.6 MB/s \n","\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n","\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["wandb.login()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"id":"fTVpZjsOvmz7","executionInfo":{"status":"ok","timestamp":1655429696094,"user_tz":-420,"elapsed":5617,"user":{"displayName":"Nguyen Hoang Gia Khang (K15 HCM)","userId":"13453407859919139403"}},"outputId":"f4bad204-7078-4565-dc5e-03772eb31a8f"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["%env WANDB_LOG_MODEL=true"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gw-S8D0-vnb0","executionInfo":{"status":"ok","timestamp":1655429696094,"user_tz":-420,"elapsed":8,"user":{"displayName":"Nguyen Hoang Gia Khang (K15 HCM)","userId":"13453407859919139403"}},"outputId":"d21f0392-b519-4725-8619-b20520e410b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["env: WANDB_LOG_MODEL=true\n"]}]},{"cell_type":"code","source":["%env LC_ALL=C.UTF-8\n","%env LANG=C.UTF-8\n","%env TRANSFORMERS_CACHE=/content/cache\n","%env HF_DATASETS_CACHE=/content/cache\n","%env CUDA_LAUNCH_BLOCKING=1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KVV8xOUU08NN","executionInfo":{"status":"ok","timestamp":1655429696095,"user_tz":-420,"elapsed":8,"user":{"displayName":"Nguyen Hoang Gia Khang (K15 HCM)","userId":"13453407859919139403"}},"outputId":"c6e5ddca-db0f-4b98-8817-a23542439e63"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["env: LC_ALL=C.UTF-8\n","env: LANG=C.UTF-8\n","env: TRANSFORMERS_CACHE=/content/cache\n","env: HF_DATASETS_CACHE=/content/cache\n","env: CUDA_LAUNCH_BLOCKING=1\n"]}]},{"cell_type":"markdown","source":["# IMPORT"],"metadata":{"id":"6vh3NJgs1ID4"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import math \n","\n","from pathlib import Path\n","from tqdm import tqdm\n","\n","import torchaudio\n","from sklearn.model_selection import train_test_split\n","\n","import os\n","import sys\n","\n","from dataclasses import dataclass\n","from typing import Optional, Tuple\n","import torch\n","from transformers.file_utils import ModelOutput\n","\n","import torch.nn as nn\n","from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n","\n","from transformers.models.wav2vec2.modeling_wav2vec2 import (\n","    Wav2Vec2PreTrainedModel,\n","    Wav2Vec2Model\n",")\n","\n","from typing import Dict, List, Optional, Union\n","\n","import transformers\n","from transformers import Wav2Vec2Processor\n","from transformers import AutoConfig\n","from datasets import DatasetDict, load_from_disk\n","\n","from typing import Any, Dict, Union\n","\n","from packaging import version\n","\n","from transformers import (\n","    Trainer,\n","    is_apex_available,\n",")\n","\n","if is_apex_available():\n","    from apex import amp\n","\n","if version.parse(torch.__version__) >= version.parse(\"1.6\"):\n","    _is_native_amp_available = True\n","    from torch.cuda.amp import autocast\n","\n","from transformers import TrainingArguments\n","\n"],"metadata":{"id":"BQdXKcau1I6t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# DATASET"],"metadata":{"id":"5gXIgeJc1RpB"}},{"cell_type":"code","source":["pooling_mode = \"mean\"\n","model_name_or_path = \"facebook/wav2vec2-base-960h\""],"metadata":{"id":"oocDJf9D1JWS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip -d /content/ /content/drive/MyDrive/CN7/DLP/voice_emotion_recognition/dataset/RAV_TESS_preprocessed_data.zip\n","!mv /content/content/RAV_TESS_preprocessed_data /content/"],"metadata":{"id":"wj8N3C7t2nDA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655429716748,"user_tz":-420,"elapsed":14252,"user":{"displayName":"Nguyen Hoang Gia Khang (K15 HCM)","userId":"13453407859919139403"}},"outputId":"8c04f705-8cc7-4b05-f07e-55bedec054ca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  /content/drive/MyDrive/CN7/DLP/voice_emotion_recognition/dataset/RAV_TESS_preprocessed_data.zip\n","   creating: /content/content/RAV_TESS_preprocessed_data/\n"," extracting: /content/content/RAV_TESS_preprocessed_data/dataset_dict.json  \n","   creating: /content/content/RAV_TESS_preprocessed_data/train/\n","  inflating: /content/content/RAV_TESS_preprocessed_data/train/dataset_info.json  \n","  inflating: /content/content/RAV_TESS_preprocessed_data/train/dataset.arrow  \n","  inflating: /content/content/RAV_TESS_preprocessed_data/train/state.json  \n","   creating: /content/content/RAV_TESS_preprocessed_data/validation/\n","  inflating: /content/content/RAV_TESS_preprocessed_data/validation/dataset_info.json  \n","  inflating: /content/content/RAV_TESS_preprocessed_data/validation/dataset.arrow  \n","  inflating: /content/content/RAV_TESS_preprocessed_data/validation/state.json  \n"]}]},{"cell_type":"code","source":["dataset = load_from_disk(\"/content/RAV_TESS_preprocessed_data\")\n","train_dataset, eval_dataset = dataset[\"train\"], dataset[\"validation\"]"],"metadata":{"id":"ZsbOZhVW3TPv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(train_dataset)\n","print(eval_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MWwLzp1Z4KOq","executionInfo":{"status":"ok","timestamp":1655429716749,"user_tz":-420,"elapsed":12,"user":{"displayName":"Nguyen Hoang Gia Khang (K15 HCM)","userId":"13453407859919139403"}},"outputId":"203648a2-34c3-4b38-80ce-31ab18ca7e12"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset({\n","    features: ['Path', 'Emotion', 'input_values', 'labels'],\n","    num_rows: 4544\n","})\n","Dataset({\n","    features: ['Path', 'Emotion', 'input_values', 'labels'],\n","    num_rows: 1136\n","})\n"]}]},{"cell_type":"code","source":["# We need to specify the input and output column\n","input_column = \"Path\"\n","output_column = \"Emotion\""],"metadata":{"id":"B4r43Rsb4WIN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# we need to distinguish the unique labels in our SER dataset\n","label_list = train_dataset.unique(output_column)\n","label_list.sort()  # Let's sort it for determinism\n","num_labels = len(label_list)\n","print(f\"A classification problem with {num_labels} classes: {label_list}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8s_O3n1j4nEL","executionInfo":{"status":"ok","timestamp":1655429748088,"user_tz":-420,"elapsed":4,"user":{"displayName":"Nguyen Hoang Gia Khang (K15 HCM)","userId":"13453407859919139403"}},"outputId":"fa58c69c-f530-4873-ff74-227c10e4bdd5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["A classification problem with 8 classes: ['angry', 'calm', 'disgusted', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n"]}]},{"cell_type":"code","source":["# config\n","config = AutoConfig.from_pretrained(\n","    model_name_or_path,\n","    num_labels=num_labels,\n","    label2id={label: i for i, label in enumerate(label_list)},\n","    id2label={i: label for i, label in enumerate(label_list)},\n","    finetuning_task=\"wav2vec2_clf\",\n",")\n","setattr(config, 'pooling_mode', pooling_mode)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["59a117098be5480fa73fb23f02aace4d","1f9a09aa2ac74be79bf4f949badd9194","caa34401fef949eeb3814d88a600df5f","aaea9a66eeff49c68f004c421ab7fd0b","dda80d4c1ce6433192da71c537bcf909","6f86a483306949208d37b157166078e7","2eed18aec69e499b854454a75a8051b9","0a4523a59a9c400c8859037a0d85b0e4","6c0070dd63f14a58ab807318a50c13e1","58f97b62f4da4e9a911e2f2513c9af62","8bd8a9c577154997aebdf88ae053f431"]},"id":"8NLxzaT12P7Y","executionInfo":{"status":"ok","timestamp":1655429750318,"user_tz":-420,"elapsed":1662,"user":{"displayName":"Nguyen Hoang Gia Khang (K15 HCM)","userId":"13453407859919139403"}},"outputId":"0aa29fc2-5d2e-4268-861a-64e22b9d53bc"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.56k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59a117098be5480fa73fb23f02aace4d"}},"metadata":{}}]},{"cell_type":"code","source":["processor = Wav2Vec2Processor.from_pretrained(model_name_or_path,)\n","target_sampling_rate = processor.feature_extractor.sampling_rate\n","print(f\"The target sampling rate: {target_sampling_rate}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":162,"referenced_widgets":["0de2b7bfae93446abec47403a58d25d8","f9ac224b9eab48088f9228152c83933f","838d752b04b148cca7675e0d49524cc5","80b00fd1482d484b9fb5722746144e2d","86a2a26fdfdc4073b3dc26ba63416421","a146cfa3953746d398fbb376b80733af","33ce54732c1744a882c619e49496c874","041123f383134ef6bc58d12f786dbe5c","971d1a4b5496434696bcfdb07ddb48f8","ac72822647d5430999b519113c727e77","ca79a02a466e47d0b5608c02fc4e452a","73ef4152219443a7ab96fecf6c3635dc","248442a2a85b41daaca2722d6d3edf29","2dcd96ef4d284909bdcd63696f0a37d4","23b8f614e3544d6eb2d9d2366bd9621a","5ed60b5370c7446eb99b81eb588086e8","c6aad1cce447495ebc7ed8689126e551","0a55efdb77604b67bae59f096792b80f","aab6a449e38f457c8e80d1d4b34f517e","0f8d54b9fcab416ea77bab388c24ef13","12f2d64730744a7eabd988465dfa3f34","a19718dda6974f4ab44a8d69f7c2a9dd","6746c51e619548b0b6e0138641ae8410","69c3c64135b64ea2bc47d26513421b8d","19c0ceca5e3645029af7d52603b909bd","f6b9932833d24a12ac56ae4435023db0","c45267bf288d4149a2892071292b781a","34c2f89df12244fe8bdfeb659d0576b0","fa4ddcbe85a849d5a40fc8d32d0a01ab","249c3f84584840aa9d3904ec2d35ae13","d5bdbe7c25e7476f989fb3d47aa2cead","b7a6786b71dc40baa65951bff2a89e8f","349aeea742aa4436bd9b39fadb7ed070","3e0db66f059c4559b5cf9a4f8068dc17","6b5fccd428484bcbbd1448964fe86b3d","efa6e1aeb4cd42a9a2deb6d74b0ed615","4e1a906c3c384f2e97fcb5ed1290033d","f2b653c16c0c43a1911ce48eca7a3de5","5fd8a23cb0324b4ca6e7422f7260bcb8","e48144f93af4499294b2c94a852f4076","d0b9fd9e5f3a4c72b12aaea51836b25c","b619623d07af4b7795f20c688292b1f8","13c993b71f794ccc94a6aa911263821c","800b35759fbe4ae29508bf9ef0d3f98c"]},"id":"upNbmj8q4wZ8","executionInfo":{"status":"ok","timestamp":1655429759460,"user_tz":-420,"elapsed":9145,"user":{"displayName":"Nguyen Hoang Gia Khang (K15 HCM)","userId":"13453407859919139403"}},"outputId":"c46bdd77-e8d3-43f9-97b3-8f3e84790cd6"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/159 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0de2b7bfae93446abec47403a58d25d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/163 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73ef4152219443a7ab96fecf6c3635dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/291 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6746c51e619548b0b6e0138641ae8410"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e0db66f059c4559b5cf9a4f8068dc17"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["The target sampling rate: 16000\n"]}]},{"cell_type":"markdown","source":["# MODEL"],"metadata":{"id":"fN8ul5_-1S0N"}},{"cell_type":"code","source":["@dataclass\n","class SpeechClassifierOutput(ModelOutput):\n","    loss: Optional[torch.FloatTensor] = None\n","    logits: torch.FloatTensor = None\n","    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n","    attentions: Optional[Tuple[torch.FloatTensor]] = None"],"metadata":{"id":"-4NI4cSa1TYT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Wav2Vec2ClassificationHead(nn.Module):\n","    \"\"\"Head for wav2vec classification task.\"\"\"\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n","        self.dropout = nn.Dropout(config.final_dropout)\n","        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n","\n","    def forward(self, features, **kwargs):\n","        x = features\n","        x = self.dropout(x)\n","        x = self.dense(x)\n","        x = torch.tanh(x)\n","        x = self.dropout(x)\n","        x = self.out_proj(x)\n","        return x\n","\n","\n","class Wav2Vec2ForSpeechClassification(Wav2Vec2PreTrainedModel):\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","        self.pooling_mode = config.pooling_mode\n","        self.config = config\n","\n","        self.wav2vec2 = Wav2Vec2Model(config)\n","        self.classifier = Wav2Vec2ClassificationHead(config)\n","\n","        self.init_weights()\n","\n","    def freeze_feature_extractor(self):\n","        self.wav2vec2.feature_extractor._freeze_parameters()\n","\n","    def merged_strategy(\n","            self,\n","            hidden_states,\n","            mode=\"mean\"\n","    ):\n","        if mode == \"mean\":\n","            outputs = torch.mean(hidden_states, dim=1)\n","        elif mode == \"sum\":\n","            outputs = torch.sum(hidden_states, dim=1)\n","        elif mode == \"max\":\n","            outputs = torch.max(hidden_states, dim=1)[0]\n","        else:\n","            raise Exception(\n","                \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n","\n","        return outputs\n","\n","    def forward(\n","            self,\n","            input_values,\n","            attention_mask=None,\n","            output_attentions=None,\n","            output_hidden_states=None,\n","            return_dict=None,\n","            labels=None,\n","    ):\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","        outputs = self.wav2vec2(\n","            input_values,\n","            attention_mask=attention_mask,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","        hidden_states = outputs[0]\n","        hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)\n","        logits = self.classifier(hidden_states)\n","\n","        loss = None\n","        if labels is not None:\n","            if self.config.problem_type is None:\n","                if self.num_labels == 1:\n","                    self.config.problem_type = \"regression\"\n","                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n","                    self.config.problem_type = \"single_label_classification\"\n","                else:\n","                    self.config.problem_type = \"multi_label_classification\"\n","\n","            if self.config.problem_type == \"regression\":\n","                loss_fct = MSELoss()\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels)\n","            elif self.config.problem_type == \"single_label_classification\":\n","                loss_fct = CrossEntropyLoss()\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            elif self.config.problem_type == \"multi_label_classification\":\n","                loss_fct = BCEWithLogitsLoss()\n","                loss = loss_fct(logits, labels)\n","\n","        if not return_dict:\n","            output = (logits,) + outputs[2:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return SpeechClassifierOutput(\n","            loss=loss,\n","            logits=logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )"],"metadata":{"id":"aqBPKs3p1Y4H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# TRAINING"],"metadata":{"id":"1Wm414tY1qqi"}},{"cell_type":"code","source":["@dataclass\n","class DataCollatorCTCWithPadding:\n","    \"\"\"\n","    Data collator that will dynamically pad the inputs received.\n","    Args:\n","        processor (:class:`~transformers.Wav2Vec2Processor`)\n","            The processor used for proccessing the data.\n","        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n","            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n","            among:\n","            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n","              sequence if provided).\n","            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n","              maximum acceptable input length for the model if that argument is not provided.\n","            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n","              different lengths).\n","        max_length (:obj:`int`, `optional`):\n","            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n","        max_length_labels (:obj:`int`, `optional`):\n","            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n","        pad_to_multiple_of (:obj:`int`, `optional`):\n","            If set will pad the sequence to a multiple of the provided value.\n","            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n","            7.5 (Volta).\n","    \"\"\"\n","\n","    processor: Wav2Vec2Processor\n","    padding: Union[bool, str] = True\n","    max_length: Optional[int] = None\n","    max_length_labels: Optional[int] = None\n","    pad_to_multiple_of: Optional[int] = None\n","    pad_to_multiple_of_labels: Optional[int] = None\n","\n","    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n","        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n","        label_features = [feature[\"labels\"] for feature in features]\n","\n","        d_type = torch.long if isinstance(label_features[0], int) else torch.float\n","\n","        batch = self.processor.pad(\n","            input_features,\n","            padding=self.padding,\n","            max_length=self.max_length,\n","            pad_to_multiple_of=self.pad_to_multiple_of,\n","            return_tensors=\"pt\",\n","        )\n","\n","        batch[\"labels\"] = torch.tensor(label_features, dtype=d_type)\n","\n","        return batch"],"metadata":{"id":"T6hWsGJl1kfq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"],"metadata":{"id":"nYJtZJ5M19eh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["is_regression = False"],"metadata":{"id":"ifLSYCvn62kL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from transformers import EvalPrediction\n","\n","\n","def compute_metrics(p: EvalPrediction):\n","    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n","    preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)\n","\n","    if is_regression:\n","        return {\"mse\": ((preds - p.label_ids) ** 2).mean().item()}\n","    else:\n","        return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}"],"metadata":{"id":"SDObGaVS64gC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Wav2Vec2ForSpeechClassification.from_pretrained(\n","    model_name_or_path,\n","    config=config,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":156,"referenced_widgets":["915c5c2e46464f5f9b79bfe79b4494c6","80c4b41c11e64edcb96b4dd7ac112b38","fc377f36a02748eba82df90d8936758b","dfce675144d64ffcab72c615409bdf09","5f9f5c0baad049d68f74970b741b3efd","0764a5036a8349f19867847237531f06","46a73beb156746c48083dac10178221e","3d37417050b24c21a8d0a7f3a8ed1eb2","8e8236edcefc43489da42dd85aad4031","73e4e2bc253f4599a782bf26b54c2cb4","d8623f826ac047da94164888b133e25a"]},"id":"yr0Ilewp7bWC","executionInfo":{"status":"ok","timestamp":1655429771534,"user_tz":-420,"elapsed":10142,"user":{"displayName":"Nguyen Hoang Gia Khang (K15 HCM)","userId":"13453407859919139403"}},"outputId":"67169fd5-e608-4cd0-ef48-980986ca2139"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/360M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"915c5c2e46464f5f9b79bfe79b4494c6"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForSpeechClassification: ['lm_head.weight', 'lm_head.bias']\n","- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'wav2vec2.masked_spec_embed']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["model.freeze_feature_extractor()"],"metadata":{"id":"w42KFlhM702P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mkdir -p wav2vec2-english-speech-emotion-recognition"],"metadata":{"id":"C_JE7ylL77HH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","training_args = TrainingArguments(\n","    report_to = 'wandb',\n","    output_dir=\"/content/wav2vec2-english-speech-emotion-recognition\",\n","    group_by_length=True,\n","    per_device_train_batch_size=64,\n","    per_device_eval_batch_size=64,\n","    gradient_accumulation_steps=2,\n","    evaluation_strategy=\"steps\",\n","    num_train_epochs=30.0,\n","    fp16=True,\n","    save_steps=10,\n","    eval_steps=10,\n","    logging_steps=10,\n","    learning_rate=1e-4,\n","    weight_decay=0.005, \n","    warmup_steps=500,\n","    save_total_limit=2,\n","    load_best_model_at_end=True,\n","    run_name = 'EVMR',\n",")"],"metadata":{"id":"9Ony0oNy76CK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer = Trainer(\n","    model=model,\n","    data_collator=data_collator,\n","    args=training_args,\n","    compute_metrics=compute_metrics,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    tokenizer=processor.feature_extractor,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zylO-Hdp8nBJ","executionInfo":{"status":"ok","timestamp":1655429807050,"user_tz":-420,"elapsed":14690,"user":{"displayName":"Nguyen Hoang Gia Khang (K15 HCM)","userId":"13453407859919139403"}},"outputId":"a2e25da4-ce44-4387-e8b5-2fba36ce621b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Using cuda_amp half precision backend\n"]}]},{"cell_type":"code","source":["from warnings import filterwarnings\n","filterwarnings(action='ignore', category=DeprecationWarning, message='`np.bool` is a deprecated alias')\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"DSqiDevG8oC-","executionInfo":{"status":"ok","timestamp":1655438698018,"user_tz":-420,"elapsed":8880095,"user":{"displayName":"Nguyen Hoang Gia Khang (K15 HCM)","userId":"13453407859919139403"}},"outputId":"5491d233-4aa7-4961-ce72-580cb9649c40"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["The following columns in the training set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 4544\n","  Num Epochs = 30\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 128\n","  Gradient Accumulation steps = 2\n","  Total optimization steps = 1050\n","Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href=\"https://wandb.me/wandb-init\" target=\"_blank\">the W&B docs</a>."]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkhangnguyen2907\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.12.18"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20220617_013808-2dpntgxv</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href=\"https://wandb.ai/khangnguyen2907/huggingface/runs/2dpntgxv\" target=\"_blank\">EVMR</a></strong> to <a href=\"https://wandb.ai/khangnguyen2907/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1050/1050 2:26:38, Epoch 29/30]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>2.088600</td>\n","      <td>2.080406</td>\n","      <td>0.147007</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>2.079500</td>\n","      <td>2.078787</td>\n","      <td>0.139965</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>2.091600</td>\n","      <td>2.075770</td>\n","      <td>0.139965</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>2.187200</td>\n","      <td>2.072041</td>\n","      <td>0.146127</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>2.074200</td>\n","      <td>2.068383</td>\n","      <td>0.162852</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>2.070800</td>\n","      <td>2.065174</td>\n","      <td>0.177817</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>2.067700</td>\n","      <td>2.061050</td>\n","      <td>0.160211</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>2.163300</td>\n","      <td>2.055563</td>\n","      <td>0.139965</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>2.062300</td>\n","      <td>2.050861</td>\n","      <td>0.138204</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>2.050300</td>\n","      <td>2.045584</td>\n","      <td>0.139085</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>2.155100</td>\n","      <td>2.017416</td>\n","      <td>0.161972</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>2.028900</td>\n","      <td>1.971374</td>\n","      <td>0.207746</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>1.975500</td>\n","      <td>1.890977</td>\n","      <td>0.310739</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>1.889200</td>\n","      <td>1.831428</td>\n","      <td>0.266725</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>1.900100</td>\n","      <td>1.710055</td>\n","      <td>0.291373</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>1.765000</td>\n","      <td>1.672454</td>\n","      <td>0.310739</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>1.690600</td>\n","      <td>1.608118</td>\n","      <td>0.404930</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>1.664800</td>\n","      <td>1.499131</td>\n","      <td>0.394366</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>1.489900</td>\n","      <td>1.433916</td>\n","      <td>0.389965</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>1.541100</td>\n","      <td>1.557169</td>\n","      <td>0.339789</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>1.522700</td>\n","      <td>1.397897</td>\n","      <td>0.514965</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>1.528600</td>\n","      <td>1.393699</td>\n","      <td>0.464789</td>\n","    </tr>\n","    <tr>\n","      <td>230</td>\n","      <td>1.444100</td>\n","      <td>1.310355</td>\n","      <td>0.456866</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>1.522400</td>\n","      <td>1.412185</td>\n","      <td>0.407570</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>1.493900</td>\n","      <td>1.248138</td>\n","      <td>0.440141</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>1.287400</td>\n","      <td>1.410411</td>\n","      <td>0.537852</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>1.322100</td>\n","      <td>1.104150</td>\n","      <td>0.617077</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>1.189600</td>\n","      <td>1.203366</td>\n","      <td>0.614437</td>\n","    </tr>\n","    <tr>\n","      <td>290</td>\n","      <td>1.231800</td>\n","      <td>1.014329</td>\n","      <td>0.664613</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>1.202600</td>\n","      <td>0.990508</td>\n","      <td>0.684859</td>\n","    </tr>\n","    <tr>\n","      <td>310</td>\n","      <td>0.981100</td>\n","      <td>0.934440</td>\n","      <td>0.674296</td>\n","    </tr>\n","    <tr>\n","      <td>320</td>\n","      <td>1.172200</td>\n","      <td>1.080008</td>\n","      <td>0.654049</td>\n","    </tr>\n","    <tr>\n","      <td>330</td>\n","      <td>1.100400</td>\n","      <td>0.862380</td>\n","      <td>0.698944</td>\n","    </tr>\n","    <tr>\n","      <td>340</td>\n","      <td>0.951500</td>\n","      <td>0.874642</td>\n","      <td>0.702465</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>1.040600</td>\n","      <td>0.831470</td>\n","      <td>0.695423</td>\n","    </tr>\n","    <tr>\n","      <td>360</td>\n","      <td>0.995300</td>\n","      <td>0.703538</td>\n","      <td>0.761444</td>\n","    </tr>\n","    <tr>\n","      <td>370</td>\n","      <td>0.892800</td>\n","      <td>0.648301</td>\n","      <td>0.756162</td>\n","    </tr>\n","    <tr>\n","      <td>380</td>\n","      <td>0.652500</td>\n","      <td>0.903192</td>\n","      <td>0.676056</td>\n","    </tr>\n","    <tr>\n","      <td>390</td>\n","      <td>0.767200</td>\n","      <td>0.642721</td>\n","      <td>0.775528</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.600500</td>\n","      <td>0.859071</td>\n","      <td>0.683099</td>\n","    </tr>\n","    <tr>\n","      <td>410</td>\n","      <td>0.717900</td>\n","      <td>0.526562</td>\n","      <td>0.823944</td>\n","    </tr>\n","    <tr>\n","      <td>420</td>\n","      <td>0.621400</td>\n","      <td>0.676815</td>\n","      <td>0.768486</td>\n","    </tr>\n","    <tr>\n","      <td>430</td>\n","      <td>0.784000</td>\n","      <td>0.993652</td>\n","      <td>0.718310</td>\n","    </tr>\n","    <tr>\n","      <td>440</td>\n","      <td>0.751000</td>\n","      <td>0.716107</td>\n","      <td>0.732394</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>0.892100</td>\n","      <td>0.598298</td>\n","      <td>0.790493</td>\n","    </tr>\n","    <tr>\n","      <td>460</td>\n","      <td>0.689600</td>\n","      <td>0.548754</td>\n","      <td>0.803697</td>\n","    </tr>\n","    <tr>\n","      <td>470</td>\n","      <td>0.474100</td>\n","      <td>0.559916</td>\n","      <td>0.801056</td>\n","    </tr>\n","    <tr>\n","      <td>480</td>\n","      <td>0.575900</td>\n","      <td>0.721548</td>\n","      <td>0.802817</td>\n","    </tr>\n","    <tr>\n","      <td>490</td>\n","      <td>0.615400</td>\n","      <td>0.544554</td>\n","      <td>0.800176</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.671400</td>\n","      <td>0.568954</td>\n","      <td>0.854753</td>\n","    </tr>\n","    <tr>\n","      <td>510</td>\n","      <td>0.582600</td>\n","      <td>0.457913</td>\n","      <td>0.855634</td>\n","    </tr>\n","    <tr>\n","      <td>520</td>\n","      <td>0.460100</td>\n","      <td>0.427635</td>\n","      <td>0.860915</td>\n","    </tr>\n","    <tr>\n","      <td>530</td>\n","      <td>0.610400</td>\n","      <td>0.947490</td>\n","      <td>0.794014</td>\n","    </tr>\n","    <tr>\n","      <td>540</td>\n","      <td>0.680800</td>\n","      <td>0.429625</td>\n","      <td>0.880282</td>\n","    </tr>\n","    <tr>\n","      <td>550</td>\n","      <td>0.417400</td>\n","      <td>0.563281</td>\n","      <td>0.842430</td>\n","    </tr>\n","    <tr>\n","      <td>560</td>\n","      <td>0.412700</td>\n","      <td>0.381011</td>\n","      <td>0.884683</td>\n","    </tr>\n","    <tr>\n","      <td>570</td>\n","      <td>0.505100</td>\n","      <td>0.338190</td>\n","      <td>0.906690</td>\n","    </tr>\n","    <tr>\n","      <td>580</td>\n","      <td>0.531200</td>\n","      <td>0.293166</td>\n","      <td>0.924296</td>\n","    </tr>\n","    <tr>\n","      <td>590</td>\n","      <td>0.452200</td>\n","      <td>0.297597</td>\n","      <td>0.904930</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.579300</td>\n","      <td>0.366046</td>\n","      <td>0.911092</td>\n","    </tr>\n","    <tr>\n","      <td>610</td>\n","      <td>0.316500</td>\n","      <td>0.254232</td>\n","      <td>0.937500</td>\n","    </tr>\n","    <tr>\n","      <td>620</td>\n","      <td>0.283300</td>\n","      <td>0.502021</td>\n","      <td>0.864437</td>\n","    </tr>\n","    <tr>\n","      <td>630</td>\n","      <td>0.288900</td>\n","      <td>0.232311</td>\n","      <td>0.934859</td>\n","    </tr>\n","    <tr>\n","      <td>640</td>\n","      <td>0.315700</td>\n","      <td>0.518093</td>\n","      <td>0.866197</td>\n","    </tr>\n","    <tr>\n","      <td>650</td>\n","      <td>0.324200</td>\n","      <td>0.239353</td>\n","      <td>0.936620</td>\n","    </tr>\n","    <tr>\n","      <td>660</td>\n","      <td>0.199100</td>\n","      <td>0.192187</td>\n","      <td>0.946303</td>\n","    </tr>\n","    <tr>\n","      <td>670</td>\n","      <td>0.398100</td>\n","      <td>0.212221</td>\n","      <td>0.937500</td>\n","    </tr>\n","    <tr>\n","      <td>680</td>\n","      <td>0.260700</td>\n","      <td>0.185688</td>\n","      <td>0.941021</td>\n","    </tr>\n","    <tr>\n","      <td>690</td>\n","      <td>0.183100</td>\n","      <td>0.408892</td>\n","      <td>0.880282</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.320500</td>\n","      <td>0.155149</td>\n","      <td>0.959507</td>\n","    </tr>\n","    <tr>\n","      <td>710</td>\n","      <td>0.156300</td>\n","      <td>0.225759</td>\n","      <td>0.939261</td>\n","    </tr>\n","    <tr>\n","      <td>720</td>\n","      <td>0.307500</td>\n","      <td>0.227009</td>\n","      <td>0.942782</td>\n","    </tr>\n","    <tr>\n","      <td>730</td>\n","      <td>0.146100</td>\n","      <td>0.231332</td>\n","      <td>0.941901</td>\n","    </tr>\n","    <tr>\n","      <td>740</td>\n","      <td>0.130900</td>\n","      <td>0.131179</td>\n","      <td>0.963908</td>\n","    </tr>\n","    <tr>\n","      <td>750</td>\n","      <td>0.127300</td>\n","      <td>0.118820</td>\n","      <td>0.972711</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.185800</td>\n","      <td>0.138834</td>\n","      <td>0.965669</td>\n","    </tr>\n","    <tr>\n","      <td>770</td>\n","      <td>0.095600</td>\n","      <td>0.115110</td>\n","      <td>0.970070</td>\n","    </tr>\n","    <tr>\n","      <td>780</td>\n","      <td>0.153000</td>\n","      <td>0.098081</td>\n","      <td>0.977113</td>\n","    </tr>\n","    <tr>\n","      <td>790</td>\n","      <td>0.122300</td>\n","      <td>0.086359</td>\n","      <td>0.983275</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.060800</td>\n","      <td>0.103138</td>\n","      <td>0.980634</td>\n","    </tr>\n","    <tr>\n","      <td>810</td>\n","      <td>0.094400</td>\n","      <td>0.087273</td>\n","      <td>0.983275</td>\n","    </tr>\n","    <tr>\n","      <td>820</td>\n","      <td>0.118200</td>\n","      <td>0.088685</td>\n","      <td>0.983275</td>\n","    </tr>\n","    <tr>\n","      <td>830</td>\n","      <td>0.050800</td>\n","      <td>0.104024</td>\n","      <td>0.976232</td>\n","    </tr>\n","    <tr>\n","      <td>840</td>\n","      <td>0.171900</td>\n","      <td>0.080443</td>\n","      <td>0.984155</td>\n","    </tr>\n","    <tr>\n","      <td>850</td>\n","      <td>0.170500</td>\n","      <td>0.127706</td>\n","      <td>0.970070</td>\n","    </tr>\n","    <tr>\n","      <td>860</td>\n","      <td>0.073000</td>\n","      <td>0.098770</td>\n","      <td>0.977993</td>\n","    </tr>\n","    <tr>\n","      <td>870</td>\n","      <td>0.032700</td>\n","      <td>0.074512</td>\n","      <td>0.983275</td>\n","    </tr>\n","    <tr>\n","      <td>880</td>\n","      <td>0.066400</td>\n","      <td>0.070020</td>\n","      <td>0.987676</td>\n","    </tr>\n","    <tr>\n","      <td>890</td>\n","      <td>0.034500</td>\n","      <td>0.074726</td>\n","      <td>0.984155</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.052600</td>\n","      <td>0.076199</td>\n","      <td>0.984155</td>\n","    </tr>\n","    <tr>\n","      <td>910</td>\n","      <td>0.036800</td>\n","      <td>0.071508</td>\n","      <td>0.985915</td>\n","    </tr>\n","    <tr>\n","      <td>920</td>\n","      <td>0.133300</td>\n","      <td>0.074113</td>\n","      <td>0.984155</td>\n","    </tr>\n","    <tr>\n","      <td>930</td>\n","      <td>0.074400</td>\n","      <td>0.067570</td>\n","      <td>0.986796</td>\n","    </tr>\n","    <tr>\n","      <td>940</td>\n","      <td>0.044800</td>\n","      <td>0.064240</td>\n","      <td>0.984155</td>\n","    </tr>\n","    <tr>\n","      <td>950</td>\n","      <td>0.027700</td>\n","      <td>0.072529</td>\n","      <td>0.986796</td>\n","    </tr>\n","    <tr>\n","      <td>960</td>\n","      <td>0.042900</td>\n","      <td>0.071544</td>\n","      <td>0.986796</td>\n","    </tr>\n","    <tr>\n","      <td>970</td>\n","      <td>0.022100</td>\n","      <td>0.068405</td>\n","      <td>0.986796</td>\n","    </tr>\n","    <tr>\n","      <td>980</td>\n","      <td>0.030200</td>\n","      <td>0.058201</td>\n","      <td>0.990317</td>\n","    </tr>\n","    <tr>\n","      <td>990</td>\n","      <td>0.021200</td>\n","      <td>0.059130</td>\n","      <td>0.985035</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.037200</td>\n","      <td>0.060581</td>\n","      <td>0.985915</td>\n","    </tr>\n","    <tr>\n","      <td>1010</td>\n","      <td>0.042200</td>\n","      <td>0.057175</td>\n","      <td>0.987676</td>\n","    </tr>\n","    <tr>\n","      <td>1020</td>\n","      <td>0.045800</td>\n","      <td>0.054626</td>\n","      <td>0.986796</td>\n","    </tr>\n","    <tr>\n","      <td>1030</td>\n","      <td>0.022900</td>\n","      <td>0.052015</td>\n","      <td>0.990317</td>\n","    </tr>\n","    <tr>\n","      <td>1040</td>\n","      <td>0.030000</td>\n","      <td>0.052371</td>\n","      <td>0.990317</td>\n","    </tr>\n","    <tr>\n","      <td>1050</td>\n","      <td>0.019000</td>\n","      <td>0.052502</td>\n","      <td>0.990317</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-10\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-10/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-10/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-10/preprocessor_config.json\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-20\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-20/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-20/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-20/preprocessor_config.json\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-30\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-30/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-30/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-30/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-10] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-40\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-40/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-40/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-40/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-20] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-50\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-50/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-50/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-50/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-30] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-60\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-60/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-60/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-60/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-40] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-70\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-70/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-70/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-70/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-50] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-80\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-80/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-80/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-80/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-60] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-90\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-90/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-90/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-90/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-70] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-100\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-100/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-100/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-100/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-80] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-110\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-110/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-110/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-110/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-90] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-120\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-120/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-120/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-120/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-100] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-130\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-130/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-130/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-130/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-110] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-140\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-140/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-140/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-140/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-120] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-150\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-150/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-150/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-150/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-130] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-160\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-160/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-160/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-160/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-140] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-170\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-170/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-170/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-170/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-150] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-180\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-180/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-180/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-180/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-160] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-190\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-190/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-190/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-190/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-170] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-200\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-200/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-200/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-200/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-180] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-210\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-210/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-210/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-210/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-190] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-220\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-220/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-220/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-220/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-200] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-230\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-230/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-230/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-230/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-210] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-240\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-240/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-240/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-240/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-220] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-250\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-250/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-250/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-250/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-230] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-260\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-260/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-260/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-260/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-240] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-270\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-270/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-270/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-270/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-250] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-280\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-280/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-280/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-280/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-260] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-290\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-290/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-290/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-290/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-270] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-300\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-300/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-300/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-300/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-280] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-310\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-310/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-310/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-310/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-290] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-320\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-320/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-320/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-320/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-300] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-330\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-330/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-330/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-330/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-310] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-340\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-340/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-340/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-340/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-320] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-350\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-350/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-350/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-350/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-330] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-360\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-360/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-360/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-360/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-340] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-370\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-370/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-370/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-370/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-350] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-380\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-380/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-380/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-380/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-360] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-390\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-390/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-390/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-390/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-370] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-400\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-400/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-400/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-400/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-380] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-410\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-410/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-410/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-410/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-390] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-420\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-420/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-420/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-420/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-400] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-430\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-430/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-430/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-430/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-420] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-440\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-440/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-440/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-440/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-430] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-450\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-450/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-450/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-450/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-440] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-460\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-460/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-460/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-460/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-450] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-470\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-470/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-470/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-470/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-460] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-480\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-480/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-480/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-480/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-470] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-490\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-490/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-490/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-490/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-480] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-500\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-500/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-500/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-500/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-490] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-510\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-510/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-510/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-510/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-410] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-520\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-520/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-520/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-520/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-500] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-530\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-530/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-530/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-530/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-510] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-540\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-540/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-540/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-540/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-530] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-550\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-550/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-550/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-550/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-540] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-560\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-560/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-560/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-560/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-520] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-570\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-570/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-570/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-570/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-550] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-580\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-580/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-580/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-580/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-560] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-590\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-590/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-590/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-590/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-570] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-600\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-600/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-600/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-600/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-590] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-610\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-610/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-610/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-610/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-580] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-620\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-620/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-620/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-620/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-600] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-630\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-630/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-630/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-630/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-610] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-640\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-640/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-640/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-640/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-620] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-650\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-650/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-650/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-650/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-640] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-660\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-660/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-660/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-660/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-630] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-670\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-670/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-670/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-670/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-650] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-680\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-680/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-680/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-680/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-660] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-690\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-690/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-690/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-690/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-670] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-700\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-700/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-700/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-700/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-680] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-710\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-710/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-710/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-710/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-690] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-720\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-720/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-720/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-720/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-710] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-730\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-730/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-730/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-730/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-720] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-740\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-740/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-740/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-740/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-700] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-750\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-750/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-750/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-750/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-730] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-760\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-760/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-760/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-760/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-740] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-770\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-770/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-770/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-770/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-750] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-780\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-780/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-780/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-780/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-760] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-790\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-790/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-790/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-790/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-770] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-800\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-800/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-800/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-800/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-780] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-810\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-810/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-810/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-810/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-800] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-820\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-820/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-820/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-820/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-810] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-830\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-830/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-830/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-830/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-820] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-840\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-840/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-840/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-840/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-790] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-850\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-850/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-850/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-850/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-830] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-860\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-860/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-860/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-860/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-850] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-870\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-870/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-870/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-870/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-840] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-880\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-880/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-880/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-880/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-860] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-890\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-890/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-890/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-890/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-870] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-900\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-900/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-900/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-900/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-890] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-910\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-910/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-910/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-910/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-900] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-920\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-920/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-920/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-920/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-910] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-930\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-930/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-930/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-930/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-880] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-940\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-940/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-940/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-940/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-920] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-950\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-950/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-950/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-950/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-930] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-960\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-960/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-960/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-960/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-950] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-970\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-970/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-970/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-970/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-960] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-980\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-980/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-980/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-980/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-940] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-990\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-990/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-990/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-990/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-970] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-1000\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-1000/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-1000/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-1000/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-990] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-1010\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-1010/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-1010/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-1010/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-980] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-1020\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-1020/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-1020/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-1020/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-1000] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-1030\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-1030/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-1030/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-1030/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-1010] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-1040\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-1040/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-1040/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-1040/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-1020] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Emotion, Path. If Emotion, Path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1136\n","  Batch size = 64\n","Saving model checkpoint to /content/wav2vec2-english-speech-emotion-recognition/checkpoint-1050\n","Configuration saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-1050/config.json\n","Model weights saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-1050/pytorch_model.bin\n","Feature extractor saved in /content/wav2vec2-english-speech-emotion-recognition/checkpoint-1050/preprocessor_config.json\n","Deleting older checkpoint [/content/wav2vec2-english-speech-emotion-recognition/checkpoint-1040] due to args.save_total_limit\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/wav2vec2-english-speech-emotion-recognition/checkpoint-1030 (score: 0.052014924585819244).\n","Using cuda_amp half precision backend\n","Saving model checkpoint to /tmp/tmpwp9b_2mw\n","Configuration saved in /tmp/tmpwp9b_2mw/config.json\n","Model weights saved in /tmp/tmpwp9b_2mw/pytorch_model.bin\n","Feature extractor saved in /tmp/tmpwp9b_2mw/preprocessor_config.json\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=1050, training_loss=0.7602961384682428, metrics={'train_runtime': 8802.0384, 'train_samples_per_second': 15.487, 'train_steps_per_second': 0.119, 'total_flos': 3.7838772599500984e+18, 'train_loss': 0.7602961384682428, 'epoch': 29.99})"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["wandb.finish()"],"metadata":{"id":"m9_pKdq30gaL","executionInfo":{"status":"ok","timestamp":1655438804791,"user_tz":-420,"elapsed":6768,"user":{"displayName":"Nguyen Hoang Gia Khang (K15 HCM)","userId":"13453407859919139403"}},"colab":{"base_uri":"https://localhost:8080/","height":879,"referenced_widgets":["c529c75724354860bc456c8a92d36b4e","bd4e056a9e9b49d69dcc45191e589d13","740adc4d80ab4b1badfd7177361b513f","7e3abf9cb3e845b8bf38f15bcdfae6ac","89d6ab92e661451a86f06376efa267d8","3be840385ef247a1967a65e3a04718a8","573ec482a42a4ef1b72857bf987a1a2c","8764da494b8843d49597cdf9cff32aa5"]},"outputId":"55736bd9-a55b-4ed4-a603-9a02fe5cb9cc"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='362.351 MB of 362.351 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0,…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c529c75724354860bc456c8a92d36b4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▁▁▁▁▂▃▃▄▃▅▅▆▆▅▇▆▆▆▇▇▇▇▇████████████████</td></tr><tr><td>eval/loss</td><td>█████▇▆▆▆▅▅▄▄▄▄▃▄▃▃▂▂▂▂▃▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>█▃▄▂▂▁▁▁▁▂▃▃▂▃▂▂▆▃▂▃▅▅▇▅▅▄▄▆▇▄▅▄▆▆█▅▆▅▅▆</td></tr><tr><td>eval/samples_per_second</td><td>▁▆▅▇▇████▇▆▆▇▆▇▇▃▆▇▆▄▄▂▄▄▅▅▃▂▅▄▅▃▃▁▄▃▄▄▃</td></tr><tr><td>eval/steps_per_second</td><td>▁▆▅▆▇█▇▇▇▆▆▆▆▆▆▆▃▆▆▆▄▄▁▄▄▅▅▂▂▄▄▅▃▃▁▄▃▄▃▂</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇████▇▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█████▇▆▆▆▆▅▅▅▄▃▃▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.99032</td></tr><tr><td>eval/loss</td><td>0.0525</td></tr><tr><td>eval/runtime</td><td>34.5897</td></tr><tr><td>eval/samples_per_second</td><td>32.842</td></tr><tr><td>eval/steps_per_second</td><td>0.52</td></tr><tr><td>train/epoch</td><td>29.99</td></tr><tr><td>train/global_step</td><td>1050</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.019</td></tr><tr><td>train/total_flos</td><td>3.7838772599500984e+18</td></tr><tr><td>train/train_loss</td><td>0.7603</td></tr><tr><td>train/train_runtime</td><td>8802.0384</td></tr><tr><td>train/train_samples_per_second</td><td>15.487</td></tr><tr><td>train/train_steps_per_second</td><td>0.119</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Synced <strong style=\"color:#cdcd00\">EVMR</strong>: <a href=\"https://wandb.ai/khangnguyen2907/huggingface/runs/2dpntgxv\" target=\"_blank\">https://wandb.ai/khangnguyen2907/huggingface/runs/2dpntgxv</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20220617_013808-2dpntgxv/logs</code>"]},"metadata":{}}]},{"cell_type":"code","source":["!mkdir -p saved_model"],"metadata":{"id":"q2pGPlJC4Gz6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.save_model(\"/content/saved_model\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mJ89GlGk194k","executionInfo":{"status":"ok","timestamp":1655439406359,"user_tz":-420,"elapsed":1346,"user":{"displayName":"Nguyen Hoang Gia Khang (K15 HCM)","userId":"13453407859919139403"}},"outputId":"0c693b40-68c1-4f2f-c1e5-429aed508fcc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/saved_model\n","Configuration saved in /content/saved_model/config.json\n","Model weights saved in /content/saved_model/pytorch_model.bin\n","Feature extractor saved in /content/saved_model/preprocessor_config.json\n"]}]},{"cell_type":"code","source":["!zip -r saved_model_evmr.zip /content/saved_model\n","!zip -r checkpoint_evmr.zip /content/wav2vec2-english-speech-emotion-recognition"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_lREFWpK45bY","executionInfo":{"status":"ok","timestamp":1655439785221,"user_tz":-420,"elapsed":137973,"user":{"displayName":"Nguyen Hoang Gia Khang (K15 HCM)","userId":"13453407859919139403"}},"outputId":"9434289f-9863-4a1f-c817-351f8d8b28de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  adding: content/saved_model/ (stored 0%)\n","  adding: content/saved_model/training_args.bin (deflated 48%)\n","  adding: content/saved_model/pytorch_model.bin (deflated 9%)\n","  adding: content/saved_model/config.json (deflated 64%)\n","  adding: content/saved_model/preprocessor_config.json (deflated 33%)\n","  adding: content/wav2vec2-english-speech-emotion-recognition/ (stored 0%)\n","  adding: content/wav2vec2-english-speech-emotion-recognition/checkpoint-1030/ (stored 0%)\n","  adding: content/wav2vec2-english-speech-emotion-recognition/checkpoint-1030/optimizer.pt (deflated 8%)\n","  adding: content/wav2vec2-english-speech-emotion-recognition/checkpoint-1030/training_args.bin (deflated 48%)\n","  adding: content/wav2vec2-english-speech-emotion-recognition/checkpoint-1030/rng_state.pth (deflated 28%)\n","  adding: content/wav2vec2-english-speech-emotion-recognition/checkpoint-1030/pytorch_model.bin (deflated 9%)\n","  adding: content/wav2vec2-english-speech-emotion-recognition/checkpoint-1030/config.json (deflated 64%)\n","  adding: content/wav2vec2-english-speech-emotion-recognition/checkpoint-1030/trainer_state.json (deflated 85%)\n","  adding: content/wav2vec2-english-speech-emotion-recognition/checkpoint-1030/scaler.pt (deflated 55%)\n","  adding: content/wav2vec2-english-speech-emotion-recognition/checkpoint-1030/scheduler.pt (deflated 49%)\n","  adding: content/wav2vec2-english-speech-emotion-recognition/checkpoint-1030/preprocessor_config.json (deflated 33%)\n","  adding: content/wav2vec2-english-speech-emotion-recognition/checkpoint-1050/ (stored 0%)\n","  adding: content/wav2vec2-english-speech-emotion-recognition/checkpoint-1050/optimizer.pt (deflated 8%)\n","  adding: content/wav2vec2-english-speech-emotion-recognition/checkpoint-1050/training_args.bin (deflated 48%)\n","  adding: content/wav2vec2-english-speech-emotion-recognition/checkpoint-1050/rng_state.pth (deflated 27%)\n","  adding: content/wav2vec2-english-speech-emotion-recognition/checkpoint-1050/pytorch_model.bin (deflated 9%)\n","  adding: content/wav2vec2-english-speech-emotion-recognition/checkpoint-1050/config.json (deflated 64%)\n","  adding: content/wav2vec2-english-speech-emotion-recognition/checkpoint-1050/trainer_state.json (deflated 85%)\n","  adding: content/wav2vec2-english-speech-emotion-recognition/checkpoint-1050/scaler.pt (deflated 55%)\n","  adding: content/wav2vec2-english-speech-emotion-recognition/checkpoint-1050/scheduler.pt (deflated 49%)\n","  adding: content/wav2vec2-english-speech-emotion-recognition/checkpoint-1050/preprocessor_config.json (deflated 33%)\n"]}]},{"cell_type":"code","source":["!zip -r wandb_evmr.zip /content/wandb"],"metadata":{"id":"7T7sPuFU6LWP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cp -av /content/wandb_evmr.zip /content/drive/MyDrive/CN7/DLP/voice_emotion_recognition/zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OtKa6e3U6RJi","executionInfo":{"status":"ok","timestamp":1655439996385,"user_tz":-420,"elapsed":472,"user":{"displayName":"Nguyen Hoang Gia Khang (K15 HCM)","userId":"13453407859919139403"}},"outputId":"7721715a-3691-4252-e233-a3094fb4d608"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["'/content/wandb_evmr.zip' -> '/content/drive/MyDrive/CN7/DLP/voice_emotion_recognition/zip/wandb_evmr.zip'\n"]}]},{"cell_type":"code","source":["!cp -av /content/saved_model_evmr.zip  /content/drive/MyDrive/CN7/DLP/voice_emotion_recognition/zip\n","!cp -av /content/checkpoint_evmr.zip  /content/drive/MyDrive/CN7/DLP/voice_emotion_recognition/zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OSiMyfcC5fZA","executionInfo":{"status":"ok","timestamp":1655439822964,"user_tz":-420,"elapsed":9422,"user":{"displayName":"Nguyen Hoang Gia Khang (K15 HCM)","userId":"13453407859919139403"}},"outputId":"c0f1693d-22a3-4585-b9c1-9b29646352a8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["'/content/saved_model_evmr.zip' -> '/content/drive/MyDrive/CN7/DLP/voice_emotion_recognition/zip/saved_model_evmr.zip'\n","'/content/checkpoint_evmr.zip' -> '/content/drive/MyDrive/CN7/DLP/voice_emotion_recognition/zip/checkpoint_evmr.zip'\n"]}]},{"cell_type":"code","source":["from transformers import AutoModel"],"metadata":{"id":"158HT4lf9lBI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Wav2Vec2ForSpeechClassification.from_pretrained(\"/content/wav2vec2-xlsr-greek-speech-emotion-recognition/checkpoint-560\")"],"metadata":{"id":"cn_gXZplxx4p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.cuda()"],"metadata":{"id":"9S7O8UuPzNR4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_trainer = Trainer(\n","    model=model,\n","    data_collator=data_collator,\n","    args=training_args,\n","    compute_metrics=compute_metrics,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    tokenizer=processor.feature_extractor,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O8jKaGI2ORiL","executionInfo":{"status":"ok","timestamp":1655395794768,"user_tz":-420,"elapsed":311,"user":{"displayName":"Nguyen Hoang Gia Khang (K15 HCM)","userId":"13453407859919139403"}},"outputId":"45e27f33-bf04-4f84-dc3f-851600162061"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Using cuda_amp half precision backend\n"]}]},{"cell_type":"code","source":["new_trainer.train(resume_from_checkpoint=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":683,"referenced_widgets":["59a2acc525e848ac870e74f3992805ea","0220a6168543473995d1aac3a9cffdda","9562084da8eb48a88a5363bc964e4a6b","b907cd9e68a144cf9e31f1274cc506ca","fe2489e977ad40e8be192947492d5e21","b3a5e8e80db6428798289d6ff4df76e8","0cea9a36a1fc4769ac29db947ccaf1f3","d7d76e25871944c992949316473f8d6e","2850648452bb4e318985b445d6139f17","f44a3e33af5c4cbfb954c78048ab63b0","0238214572d44ca391dec89ae3f25be1"]},"id":"Gq67HuoqOaVC","executionInfo":{"status":"ok","timestamp":1655395913305,"user_tz":-420,"elapsed":117180,"user":{"displayName":"Nguyen Hoang Gia Khang (K15 HCM)","userId":"13453407859919139403"}},"outputId":"88541a3d-bc63-475d-9c4d-6c8feaf77e15"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Loading model from /content/wav2vec2-xlsr-greek-speech-emotion-recognition/checkpoint-560.\n","The following columns in the training set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: Path, Emotion. If Path, Emotion are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 4544\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 2\n","  Total optimization steps = 568\n","  Continuing training from checkpoint, will skip to saved global_step\n","  Continuing training from epoch 0\n","  Continuing training from global step 560\n","  Will skip the first 0 epochs then the first 1120 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1120 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59a2acc525e848ac870e74f3992805ea"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href=\"https://wandb.me/wandb-init\" target=\"_blank\">the W&B docs</a>."]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.12.18"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20220616_160957-2jho3eyf</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href=\"https://wandb.ai/khangnguyen2907/huggingface/runs/2jho3eyf\" target=\"_blank\">EVMR</a></strong> to <a href=\"https://wandb.ai/khangnguyen2907/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='568' max='568' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [568/568 00:10, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Using cuda_amp half precision backend\n","Saving model checkpoint to /tmp/tmpq501g8li\n","Configuration saved in /tmp/tmpq501g8li/config.json\n","Model weights saved in /tmp/tmpq501g8li/pytorch_model.bin\n","Feature extractor saved in /tmp/tmpq501g8li/preprocessor_config.json\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=568, training_loss=0.020683181117957746, metrics={'train_runtime': 109.6715, 'train_samples_per_second': 41.433, 'train_steps_per_second': 5.179, 'total_flos': 1.570500384537744e+17, 'train_loss': 0.020683181117957746, 'epoch': 1.0})"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":[""],"metadata":{"id":"rsGwZWiXOgAr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation"],"metadata":{"id":"ucomN91OMzeE"}},{"cell_type":"code","source":["import librosa\n","from sklearn.metrics import classification_report"],"metadata":{"id":"oUJ2NfvTM8-u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = load_from_disk(\"/content/RAV_TESS_preprocessed_data\")\n","test_dataset = dataset[\"validation\"]"],"metadata":{"id":"6h5Nzd-mNBMZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Device: {device}\")"],"metadata":{"id":"kRB05zIGNfVD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name_or_path = \"\"\n","config  = AutoConfig(model_name_or_path)\n","processor = Wav2Vec2Processor.from_pretrained(model_name_or_path)\n","model = Wav2Vec2ForSpeechClassification.from_pretrained(model_name_or_path).to(device)"],"metadata":{"id":"ZVTnhiq6NgPt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def speech_file_to_array_fn(batch):\n","    speech_array, sampling_rate = torchaudio.load(batch[\"Path\"])\n","    speech_array = speech_array.squeeze().numpy()\n","    speech_array = librosa.resample(np.asarray(speech_array), sampling_rate, processor.feature_extractor.sampling_rate)\n","\n","    batch[\"speech\"] = speech_array\n","    return batch\n","\n","\n","def predict(batch):\n","    features = processor(batch[\"speech\"], sampling_rate=processor.feature_extractor.sampling_rate, return_tensors=\"pt\", padding=True)\n","\n","    input_values = features.input_values.to(device)\n","    attention_mask = features.attention_mask.to(device)\n","\n","    with torch.no_grad():\n","        logits = model(input_values, attention_mask=attention_mask).logits \n","\n","    pred_ids = torch.argmax(logits, dim=-1).detach().cpu().numpy()\n","    batch[\"predicted\"] = pred_ids\n","    return batch"],"metadata":{"id":"SqX-RKzqQXqq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_dataset = test_dataset.map(speech_file_to_array_fn)"],"metadata":{"id":"2MISslcdRDUm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = test_dataset.map(predict, batched=True, batch_size=8)"],"metadata":{"id":"jMzpz2tiREZx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label_names = [config.id2label[i] for i in range(config.num_labels)]\n","label_names"],"metadata":{"id":"TpatC63BQE_E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label_names = [config.id2label[i] for i in range(config.num_labels)]\n","label_names"],"metadata":{"id":"qFx2zIeLRIrz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_true = [config.label2id[name] for name in result[\"emotion\"]]\n","y_pred = result[\"predicted\"]\n","\n","print(y_true[:5])\n","print(y_pred[:5])"],"metadata":{"id":"3it9H-6XRKTR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(classification_report(y_true, y_pred, target_names=label_names))"],"metadata":{"id":"MryPywugRNUk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Prediction "],"metadata":{"id":"-vWNxzatRQwJ"}},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model_name_or_path = \"\"\n","config = AutoConfig.from_pretrained(model_name_or_path)\n","processor = Wav2Vec2Processor.from_pretrained(model_name_or_path)\n","sampling_rate = processor.feature_extractor.sampling_rate\n","model = Wav2Vec2ForSpeechClassification.from_pretrained(model_name_or_path).to(device)"],"metadata":{"id":"k9DNtZygRSG0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def speech_file_to_array_fn(path, sampling_rate):\n","    speech_array, _sampling_rate = torchaudio.load(path)\n","    resampler = torchaudio.transforms.Resample(_sampling_rate)\n","    speech = resampler(speech_array[0]).squeeze().numpy()\n","    return speech\n","\n","\n","def predict(path, sampling_rate):\n","    speech = speech_file_to_array_fn(path, sampling_rate)\n","    features = processor(speech, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n","\n","    input_values = features.input_values.to(device)\n","    attention_mask = features.attention_mask.to(device)\n","\n","    with torch.no_grad():\n","        logits = model(input_values, attention_mask=attention_mask).logits\n","\n","    scores = F.softmax(logits, dim=1).detach().cpu().numpy()[0]\n","    outputs = [{\"Emotion\": config.id2label[i], \"Score\": f\"{round(score * 100, 3):.1f}%\"} for i, score in enumerate(scores)]\n","    return outputs\n","\n","STYLES = \"\"\"\n","<style>\n","div.display_data {\n","    margin: 0 auto;\n","    max-width: 500px;\n","}\n","table.xxx {\n","    margin: 50px !important;\n","    float: right !important;\n","    clear: both !important;\n","}\n","table.xxx td {\n","    min-width: 300px !important;\n","    text-align: center !important;\n","}\n","</style>\n","\"\"\".strip()\n","\n","def prediction(df_row):\n","    path, emotion = df_row[\"Path\"], df_row[\"Emotion\"]\n","    df = pd.DataFrame([{\"Emotion\": emotion, \"Sentence\": \"    \"}])\n","    setup = {\n","        'border': 2,\n","        'show_dimensions': True,\n","        'justify': 'center',\n","        'classes': 'xxx',\n","        'escape': False,\n","    }\n","    ipd.display(ipd.HTML(STYLES + df.to_html(**setup) + \"<br />\"))\n","    speech, sr = torchaudio.load(path)\n","    speech = speech[0].numpy().squeeze()\n","    speech = librosa.resample(np.asarray(speech), sr, sampling_rate)\n","    ipd.display(ipd.Audio(data=np.asarray(speech), autoplay=True, rate=sampling_rate))\n","\n","    outputs = predict(path, sampling_rate)\n","    r = pd.DataFrame(outputs)\n","    ipd.display(ipd.HTML(STYLES + r.to_html(**setup) + \"<br />\"))"],"metadata":{"id":"WL5sOipVRdJE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test = pd.read_csv(\"/content/data/test.csv\", sep=\"\\t\")\n","test.head()"],"metadata":{"id":"JzavqB0ZTkD-"},"execution_count":null,"outputs":[]}]}